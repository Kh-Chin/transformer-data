{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"machine_shape":"hm","gpuType":"V100","authorship_tag":"ABX9TyM6T+g7BRCQ0r+1GsYzEXsR"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"PXoMGRlZCJYv"},"outputs":[],"source":["# 准备环境\n","!pip install transformers\n","!pip install datasets\n","!pip install tensorflow\n","!pip install nltk==3.5\n","!pip install evaluate"]},{"cell_type":"code","source":["# 获取上传的数据集\n","!git clone https://github.com/Kh-Chin/transformer-data.git"],"metadata":{"id":"yyRfNgPHE0Kt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 将每个领域数据集进行合并\n","import numpy as np\n","import pandas as pd\n","import glob\n","import os\n","\n","\n","path = '/content/transformer-data'\n","all_files = glob.glob(os.path.join(path , \"*.csv\"))\n","\n","li = []\n","\n","for filename in all_files:\n","    file = pd.read_csv(filename, index_col=0, sep=\"|\")\n","    print(f'file {filename} read!')\n","    li.append(file)\n","\n","raw_df = pd.concat(li, axis=0, ignore_index=True)\n","print(\"Read successful!\")\n"],"metadata":{"id":"2MY4b_UJG_k3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["raw_df.head()"],"metadata":{"id":"h_LCj7q1qxbs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# df = df.dropna()\n","# df"],"metadata":{"id":"ApdB_AcGWhng"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = raw_df.drop_duplicates()\n","df.dropna(how='all')\n","df = df[~df['Job desc'].isna()]"],"metadata":{"id":"KbQruRa0_Oah"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.shape"],"metadata":{"id":"fgq1ZsLS_9PW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df['cleaner_Job_desc'] = None\n","df['Input'] = None\n","df['text'] = None\n","df['cleaner_text'] = None"],"metadata":{"id":"JxrHsrGe6n-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 数据预处理，与输入构建\n","import re\n","def preprocessing(df):\n","  pat = r\"^About us:.*?\\n(?=(Responsibilities:|Education, Qualifications & Experience:|Skills & Abilities:|Other:|About us:|\\Z))\"\n","  for index, row in df.iterrows():\n","    df.loc[index, 'cleaner_Job_desc'] = re.sub(pat, \"\", row['Job desc'], flags=re.I|re.M|re.S)\n","    df.loc[index, 'Input'] = f\"<bos>\\nJob Description for {row['Job title']} which uses {row['Skills']} skill:\\n<desc>\\n\"\n","    df.loc[index, 'text'] = row['Input'] + row['Job desc'] + \"\\n<eos>\"\n","    df.loc[index, 'cleaner_text'] = row['Input'] + row['cleaner_Job_desc'] + \"\\n<eos>\"\n","  return df"],"metadata":{"id":"ofzY04DQ2IJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = preprocessing(df)"],"metadata":{"id":"Qp0eyBvp5Tms"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df.columns"],"metadata":{"id":"Nmgb7Qsv5vlE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(df['text'][106])\n","print(df['cleaner_text'][106])\n","print(df['Input'][106])"],"metadata":{"id":"jdjtjcXH4XnS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from datasets import Dataset\n","clean_df = df[['Input','text']]\n","\n","clean_df = Dataset.from_pandas(clean_df)\n","print(clean_df)"],"metadata":{"id":"Xu2_RwvyQUjm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 构建编码层和模型\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertLMHeadModel, GPT2Tokenizer, TFGPT2LMHeadModel\n","\n","SPECIAL_TOKENS_MAPPING = {\n","    'bos_token': '<bos>',\n","    'eos_token': '<eos>',\n","    'pad_token': '<pad>',\n","    'additional_special_tokens': ['<desc>']\n","}\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='right')\n","model = TFGPT2LMHeadModel.from_pretrained('gpt2')\n","\n","orig_num_tokens = len(tokenizer.get_vocab())\n","num_special_tokens = tokenizer.add_special_tokens(SPECIAL_TOKENS_MAPPING)\n","\n","model.resize_token_embeddings(new_num_tokens=orig_num_tokens + num_special_tokens)"],"metadata":{"id":"b3G0uTVV9cHu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 进行输入编码\n","import psutil\n","\n","def tokenize_dataset(data):\n","    # Keys of the returned dictionary will be added to the dataset as columns\n","    input_tokens = tokenizer(data['text'], truncation=True, max_length=1024, padding='max_length')\n","    return {'input_ids': input_tokens['input_ids'],\n","            'labels': input_tokens['input_ids'], \n","            'attention_mask': input_tokens['attention_mask']\n","    }\n","CPU_COUNT = psutil.cpu_count()\n","tokenized_df = clean_df.map(tokenize_dataset, batched=True, num_proc=CPU_COUNT)\n","# tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)\n","print(tokenized_df)"],"metadata":{"id":"UfDJN71f96dz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(len(tokenized_df['labels'][-1]))\n","print(tokenized_df['input_ids'][0])"],"metadata":{"id":"yeaHaTrLul8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 数据集划分\n","split_df = tokenized_df.train_test_split(test_size=0.2)\n","print(split_df)"],"metadata":{"id":"tPJqeY4XmH_T"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install huggingface_hub\n","!huggingface-cli login"],"metadata":{"id":"0COSZOtY9JEE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_df.push_to_hub('keehuachin/clean')"],"metadata":{"id":"1XXDJZke9dsz"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["split_df['test']['text'][0]"],"metadata":{"id":"0HBOyoo-0cNY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","split_df = datasets.load_dataset(\"keehuachin/clean\")"],"metadata":{"id":"TiF-SfQzAqQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 数据集转化与预备\n","train_df = split_df['train'].to_tf_dataset(\n","    columns=['input_ids', 'attention_mask'],\n","    label_cols=['labels'],\n","    shuffle=True,\n","    batch_size=8,\n","    drop_remainder=True\n",")\n","\n","test_df = split_df['test'].to_tf_dataset(\n","    columns=['input_ids', 'attention_mask'],\n","    label_cols=['labels'],\n","    shuffle=False,\n","    batch_size=8,\n","    drop_remainder=True\n",")"],"metadata":{"id":"d-eid5M5Y2_-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(train_df)\n","print(test_df)"],"metadata":{"id":"nN94g26ZkQbl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rouge_score\n","!pip install deepspeed"],"metadata":{"id":"LyRiHKu-tsvS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 模型训练\n","optimizer=tf.keras.optimizers.Adam(3e-5)\n","model.compile(\n","\toptimizer=optimizer)\n","\n","tf.compat.v1.ConfigProto(device_count = {'GPU': len(tf.config.experimental.list_physical_devices('GPU')) , 'CPU': CPU_COUNT})\n","\n","model.fit(train_df, validation_data=test_df, epochs=5, batch_size=8)"],"metadata":{"id":"H7FER7HgUYcE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"mnC7PDT6BVCh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content/drive/My Drive/Colab Notebooks/\n","model.save_pretrained(\"keehua-gpt2-final-clean\", from_pt=True)"],"metadata":{"id":"0yOSbCBlER1u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 将模型进行存取和读取\n","import tensorflow as tf\n","from transformers import BertTokenizer, TFBertLMHeadModel, GPT2Tokenizer, TFGPT2LMHeadModel\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/Colab Notebooks/\n","\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')\n","t_model = TFGPT2LMHeadModel.from_pretrained('/content/drive/My Drive/Colab Notebooks/keehua-gpt2-all-final-1', pad_token_id=tokenizer.eos_token_id)\n","SPECIAL_TOKENS_MAPPING = {\n","    'bos_token': '<bos>',\n","    'eos_token': '<eos>',\n","    'pad_token': '<pad>',\n","    'additional_special_tokens': ['<desc>']\n","}\n","\n","orig_num_tokens = len(tokenizer.get_vocab())\n","num_special_tokens = tokenizer.add_special_tokens(SPECIAL_TOKENS_MAPPING)"],"metadata":{"id":"QSdiJMp9Ad3R"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!pip install rouge_score"],"metadata":{"id":"5NHUCiZpeSfF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import datasets\n","split_df = datasets.load_dataset(\"keehuachin/clean\")\n","split_df"],"metadata":{"id":"53hNM1QkW1OV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import TFGPT2LMHeadModel, GPT2Tokenizer\n","import datasets\n","from tqdm import tqdm\n","import evaluate\n","\n","path = '/content/drive/My Drive/Colab Notebooks/keehua-gpt2-final-clean'\n","tokenizer = GPT2Tokenizer.from_pretrained('gpt2', padding_side='left')\n","tokenizer.pad_token_id = tokenizer.eos_token_id\n","SPECIAL_TOKENS_MAPPING = {\n","    'bos_token': '<bos>',\n","    'eos_token': '<eos>',\n","    'pad_token': '<pad>',\n","    'additional_special_tokens': ['<desc>']\n","}\n","\n","orig_num_tokens = len(tokenizer.get_vocab())\n","num_special_tokens = tokenizer.add_special_tokens(SPECIAL_TOKENS_MAPPING)\n","model = TFGPT2LMHeadModel.from_pretrained(path, pad_token_id=tokenizer.eos_token_id)\n","\n","split_df = datasets.load_dataset(\"keehuachin/clean\")\n","\n","# 模型评价指标计算\n","pred_list = []\n","ref_list = []\n","df = split_df['test']\n","meteor = evaluate.load(\"meteor\")\n","bleu = evaluate.load(\"bleu\")\n","rouge = evaluate.load(\"rouge\")\n","for i in range(len(df)):\n","    tokens = tokenizer(df['Input'][i], truncation=True, max_length=128, padding='max_length', return_tensors='tf')\n","    pred = model.generate(tokens['input_ids'],\n","                        max_length = 1024,\n","                        no_repeat_ngram_size = 3,\n","                        early_stopping = True\n","                       )\n","    pred_list.append(tokenizer.decode(pred[0], skip_special_tokens=True))\n","    ref_list.append(df['text'][i])\n","    print(f\"{i} done!\")\n","\n","\n"],"metadata":{"id":"ZCYfXf5WVbyr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["meteor_score = meteor.compute(predictions=pred_list, references=ref_list)['meteor']\n","bleu_score = bleu.compute(predictions=pred_list, references=[[i] for i in ref_list])['bleu']\n","rouge_score = rouge.compute(predictions=pred_list, references=ref_list)\n","\n","print(f\"\"\"\n","meteor_score: {meteor_score}\n","bleu_score: {bleu_score}\n","rouge_score: {rouge_score}\n","\"\"\")"],"metadata":{"id":"Vv_qJwr607W8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","output = pd.DataFrame([pred_list, ref_list]).transpose()\n","for i in range(5):\n","\n","  print(f\"{i} Pred:\")\n","  print(output.loc[i,0])\n","  print(f\"{i} True;\")\n","  print(output.loc[i,1])\n","\n","  print(\"Done!\")"],"metadata":{"id":"Bh1H0Q0C2KND"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 预测成果存取\n","file_path = \"/content/drive/MyDrive/Colab Notebooks/clean_pred.csv\"\n","output.to_csv(file_path)"],"metadata":{"id":"WaVid_4M3nYw"},"execution_count":null,"outputs":[]}]}